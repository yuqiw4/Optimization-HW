{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLY 561 HW\n",
    "\n",
    "Name:Yuqi Wang   \n",
    "NetID:yw545"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Prove that\n",
    "$$\n",
    "\\nabla (f \\circ g)(x,y) = Dg(x,y)^T \\nabla f(g(x,y))\n",
    "$$\n",
    "using the chain rule\n",
    "$$\n",
    "(f \\circ \\gamma)'(t) = \\nabla (f(\\gamma(t))^T \\gamma '(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROOF: **  \n",
    "$$\n",
    "\\nabla (f \\circ g)(x,y) = \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial (f \\circ g)(x,y)}{\\partial x} \\\\ \\frac{\\partial (f \\circ g)(x,y)}{\\partial y}\n",
    "\\end{pmatrix} =   \n",
    "\\begin{pmatrix}\n",
    "\\nabla f(g(x,y))^T \\frac{\\partial g(x,y)}{\\partial x} \\\\ \\nabla f(g(x,y))^T \\frac{\\partial g(x,y)}{\\partial y} \n",
    "\\end{pmatrix} \\text{ (Using the chain rule) }\n",
    "$$\n",
    "Since \n",
    "$$\n",
    "\\nabla f(g(x,y))^T  = \\begin{pmatrix} \\frac{\\partial f(g(x,y))}{\\partial g_1(x,y)} & \\frac{\\partial f(g(x,y))}{\\partial g_2(x,y)} \\end{pmatrix} \\text{   and  } \\\\\n",
    "\\frac{\\partial g(x,y)}{\\partial x}  = \\begin{pmatrix} \\frac{\\partial g_1(x,y)}{\\partial x} \\\\ \\frac{\\partial g_2(x,y)}{\\partial x} \\end{pmatrix}, \n",
    "\\frac{\\partial g(x,y)}{\\partial y} = \\begin{pmatrix} \\frac{\\partial g_1(x,y)}{\\partial y} \\\\ \\frac{\\partial g_2(x,y)}{\\partial y} \\end{pmatrix}\n",
    "$$\n",
    "Then, \n",
    "$$\n",
    "\\begin{pmatrix}\\nabla f(g(x,y))^T \\frac{\\partial g(x,y)}{\\partial x} \\\\ \\nabla f(g(x,y))^T \\frac{\\partial g(x,y)}{\\partial y} \\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "\\begin{pmatrix} \\frac{\\partial f(g(x,y))}{\\partial g_1(x,y)} & \\frac{\\partial f(g(x,y))}{\\partial g_2(x,y)} \\end{pmatrix} \n",
    "\\begin{pmatrix} \\frac{\\partial g_1(x,y)}{\\partial x} \\\\ \\frac{\\partial g_2(x,y)}{\\partial x} \\end{pmatrix} \n",
    "\\\\ \n",
    "\\begin{pmatrix} \\frac{\\partial f(g(x,y))}{\\partial g_1(x,y)} & \\frac{\\partial f(g(x,y))}{\\partial g_2(x,y)} \\end{pmatrix} \n",
    "\\begin{pmatrix} \\frac{\\partial g_1(x,y)}{\\partial y} \\\\ \\frac{\\partial g_2(x,y)}{\\partial y} \\end{pmatrix} \\end{pmatrix} \\\\\n",
    "= \\begin{pmatrix} \n",
    "\\frac{\\partial f(g(x,y))}{\\partial g_1(x,y)} \\frac{\\partial g_1(x,y)}{\\partial x} + \\frac{\\partial f(g(x,y))}{\\partial g_2(x,y)} \\frac{\\partial g_2(x,y)}{\\partial x}\n",
    "\\\\\n",
    "\\frac{\\partial f(g(x,y))}{\\partial g_1(x,y)} \\frac{\\partial g_1(x,y)}{\\partial y} + \\frac{\\partial f(g(x,y))}{\\partial g_2(x,y)} \\frac{\\partial g_2(x,y)}{\\partial y}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "For the right side,  \n",
    "$$\n",
    "Dg(x,y)^T \\nabla f(g(x,y)) = \\begin{pmatrix} \\frac{\\partial g_1(x,y)}{\\partial x} & \\frac{\\partial g_2(x,y)}{\\partial x} \\\\ \\frac{\\partial g_1(x,y)}{\\partial y} & \\frac{\\partial g_2(x,y)}{\\partial y} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial f(g(x,y))}{\\partial g_1(x,y)} \\\\ \\frac{\\partial f(g(x,y))}{\\partial g_2(x,y)} \\end{pmatrix} = \\begin{pmatrix} \n",
    "\\frac{\\partial f(g(x,y))}{\\partial g_1(x,y)} \\frac{\\partial g_1(x,y)}{\\partial x} + \\frac{\\partial f(g(x,y))}{\\partial g_2(x,y)} \\frac{\\partial g_2(x,y)}{\\partial x}\n",
    "\\\\\n",
    "\\frac{\\partial f(g(x,y))}{\\partial g_1(x,y)} \\frac{\\partial g_1(x,y)}{\\partial y} + \\frac{\\partial f(g(x,y))}{\\partial g_2(x,y)} \\frac{\\partial g_2(x,y)}{\\partial y}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Therefore, $\\nabla (f \\circ g)(x,y) = Dg(x,y)^T \\nabla f(g(x,y))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "### Part (a)\n",
    "\n",
    "$$\n",
    "\\ell_{lin}(\\beta_0,\\beta_1) = \\frac{1}{10} \\sum_{i=1}^{10} (y_i - \\beta_1x_i - \\beta_0)^2\n",
    "$$\n",
    "We can calculate $\\nabla ^2\\ell_{lin}$ first, and then varify it's a positive definite matrix. \n",
    "$$\n",
    "\\nabla ^2\\ell_{lin} = \\begin{pmatrix} \n",
    "\\frac{\\partial^2 \\ell_{lin}}{\\partial^2 \\beta_0} & \\frac{\\partial^2 \\ell_{lin}}{\\partial \\beta_0 \\partial \\beta_1}\\\\ \n",
    "\\frac{\\partial^2 \\ell_{lin}}{\\partial \\beta_0 \\partial \\beta_1} & \\frac{\\partial^2 \\ell_{lin}}{\\partial^2 \\beta_1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Since\n",
    "$$\n",
    "\\frac{\\partial^2 \\ell_{lin}}{\\partial^2 \\beta_0} = \\frac{\\partial^2 }{\\partial^2 \\beta_0}\\frac{1}{10} \\sum_{i=1}^{10} (y_i - \\beta_1x_i - \\beta_0)^2 = \\frac{1}{10} \\sum_{i=1}^{10}\\frac{\\partial^2 }{\\partial^2 \\beta_0}(y_i - \\beta_1x_i - \\beta_0)^2\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{10} \\sum_{i=1}^{10} \\frac{\\partial }{\\partial \\beta_0}-2(y_i - \\beta_1x_i - \\beta_0) = \\frac{1}{10} \\sum_{i=1}^{10} 2 = 2 \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial^2 \\ell_{lin}}{\\partial^2 \\beta_1} = \\frac{\\partial^2 }{\\partial^2 \\beta_1}\\frac{1}{10} \\sum_{i=1}^{10} (y_i - \\beta_1x_i - \\beta_0)^2 = \\frac{1}{10} \\sum_{i=1}^{10}\\frac{\\partial^2 }{\\partial^2 \\beta_1}(y_i - \\beta_1x_i - \\beta_0)^2\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{10} \\sum_{i=1}^{10} \\frac{\\partial }{\\partial \\beta_0}-2x_i(y_i - \\beta_1x_i - \\beta_0) = \\frac{1}{10} \\sum_{i=1}^{10} 2x_i^2 = \\frac{7}{5}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial^2 \\ell_{lin}}{\\partial \\beta_0 \\partial \\beta_1} = \\frac{\\partial^2 }{\\partial \\beta_0 \\partial \\beta_1}\\frac{1}{10} \\sum_{i=1}^{10} (y_i - \\beta_1x_i - \\beta_0)^2 = \\frac{1}{10} \\sum_{i=1}^{10}\\frac{\\partial^2 }{\\partial \\beta_0 \\partial \\beta_1}(y_i - \\beta_1x_i - \\beta_0)^2\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{10} \\sum_{i=1}^{10} \\frac{\\partial }{\\partial \\beta_1}-2(y_i - \\beta_1x_i - \\beta_0)= \\frac{1}{10} \\sum_{i=1}^{10} 2x_i = \\frac{1}{5}\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\nabla ^2\\ell_{lin} = \\begin{pmatrix} \n",
    "2 & \\frac{1}{5} \\\\ \n",
    "\\frac{1}{5} & \\frac{7}{5}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Then we calculate the determinant of $\\nabla ^2\\ell_{lin}$ to prove $\\nabla ^2\\ell_{lin}$ is a positive definite matrix.\n",
    "$$\n",
    "\\text{det}(\\nabla ^2\\ell_{lin}) = \\text{det}\\begin{pmatrix} \n",
    "2 & \\frac{1}{5} \\\\ \n",
    "\\frac{1}{5} & \\frac{7}{5}\n",
    "\\end{pmatrix} = \\frac{14}{5} - \\frac{1}{25} = \\frac{69}{25} > 0\n",
    "$$\n",
    "And obviously, $2>0$. Therefore, the Sum of Square Errors function, $\\ell_{lin}(\\beta_0,\\beta_1) = \\frac{1}{10} \\sum_{i=1}^{10} (y_i - \\beta_1x_i - \\beta_0)^2$ is strictly convex by the Second Order Conditions for Convexity.  \n",
    "\n",
    "**The unique minimizer**  \n",
    "\n",
    "$\\ell_{lin}(\\beta_0,\\beta_1)$ is strictly convex, so there is a unique minimizer $(\\beta_0^\\ast , \\beta_1^\\ast)$ must satisfy the condition\n",
    "$$\n",
    "\\nabla \\ell_{lin}(\\beta_0^\\ast , \\beta_1^\\ast) = \\begin{pmatrix} \n",
    "\\partial_1 \\ell_{lin}(\\beta_0^\\ast , \\beta_1^\\ast) \\\\ \n",
    "\\partial_2 \\ell_{lin}(\\beta_0^\\ast , \\beta_1^\\ast)\n",
    "\\end{pmatrix} = {\\bf 0}\n",
    "$$\n",
    "Then,  \n",
    "$$\n",
    "\\partial_1 \\ell_{lin}(\\beta_0^\\ast , \\beta_1^\\ast) = \\frac{1}{10} \\sum_{i=1}^{10} -2(y_i - \\beta_1^\\ast x_i - \\beta_0^\\ast) = 0\n",
    "$$\n",
    "$$\n",
    "\\partial_2 \\ell_{lin}(\\beta_0^\\ast , \\beta_1^\\ast) = \\frac{1}{10} \\sum_{i=1}^{10} -2x_i(y_i - \\beta_1^\\ast x_i - \\beta_0^\\ast) = 0\n",
    "$$\n",
    "We get\n",
    "$$\n",
    "\\beta_1^\\ast + 10 \\beta_0^\\ast = 0\n",
    "$$\n",
    "$$\n",
    "5 - 7 \\beta_1^\\ast  -\\beta_0^\\ast = 0\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\beta_0^\\ast = -\\frac{5}{69}\n",
    "$$\n",
    "$$\n",
    "\\beta_1^\\ast = \\frac{50}{69}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)\n",
    "\n",
    "$$\n",
    "\\ell_{log}(\\beta_0,\\beta_1) = \\frac{1}{10} \\sum_{i=1}^{10} log(1+ e^{-y_i (\\beta_1x_i + \\beta_0)})\n",
    "$$\n",
    "Since\n",
    "$$\n",
    "\\frac{\\partial^2 \\ell_{log}}{\\partial^2 \\beta_0} = \\frac{1}{10} \\sum_{i=1}^{10}\\frac{\\partial^2 }{\\partial^2 \\beta_0}log(1+ e^{-y_i (\\beta_1x_i + \\beta_0)})\n",
    "$$\n",
    "$$\n",
    "=\\frac{1}{10} \\sum_{i=1}^{10} \\frac{\\partial}{\\partial \\beta_0} -y_i +\\frac{y_i}{1+e^{-y_i (\\beta_1x_i + \\beta_0)}} = \\frac{1}{10} \\sum_{i=1}^{10} \\frac{y_i^2 e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2} \\\\\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial^2 \\ell_{log}}{\\partial^2 \\beta_1} = \\frac{1}{10} \\sum_{i=1}^{10}\\frac{\\partial^2 }{\\partial^2 \\beta_1}log(1+ e^{-y_i (\\beta_1x_i + \\beta_0)})\n",
    "$$\n",
    "$$\n",
    "=\\frac{1}{10} \\sum_{i=1}^{10} \\frac{\\partial}{\\partial \\beta_1} -y_i x_i +\\frac{y_i x_i}{1+e^{-y_i (\\beta_1x_i + \\beta_0)}} = \\frac{1}{10} \\sum_{i=1}^{10} \\frac{y_i^2 x_i^2 e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2} \\\\\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial^2 \\ell_{log}}{\\partial \\beta_0 \\partial \\beta_1} = \\frac{1}{10} \\sum_{i=1}^{10}\\frac{\\partial^2 }{\\partial \\beta_0 \\partial \\beta_1} log(1+ e^{-y_i (\\beta_1x_i + \\beta_0)})\n",
    "$$\n",
    "$$\n",
    "=\\frac{1}{10} \\sum_{i=1}^{10} \\frac{\\partial }{\\partial \\beta_1} -y_i +\\frac{y_i}{1+e^{-y_i (\\beta_1x_i + \\beta_0)}} = \\frac{1}{10} \\sum_{i=1}^{10} \\frac{y_i^2 x_i e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2}\n",
    "$$\n",
    "Let's denote that\n",
    "$$\n",
    "\\nabla ^2\\ell_{log} = \\begin{pmatrix} \n",
    "\\frac{1}{10} \\sum_{i=1}^{10} \\frac{y_i^2 e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2} & \\frac{1}{10} \\sum_{i=1}^{10} \\frac{y_i^2 x_i e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2} \\\\ \n",
    "\\frac{1}{10} \\sum_{i=1}^{10} \\frac{y_i^2 x_i e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2} & \\frac{1}{10} \\sum_{i=1}^{10} \\frac{y_i^2 x_i^2 e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2} \n",
    "\\end{pmatrix} = \\begin{pmatrix} \n",
    "a_{(1,1)} & a_{(1,2)} \\\\ \n",
    "a_{(1,2)} & a_{(2,2)}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Firstly, it is obvious that $a_{(1,1)}>0$ since $y_i^2 = 1> 0$ for all $i$ and the range of exponential function is $(0,\\infty)$.\n",
    "\n",
    "Then we calculate the determinant of $\\nabla ^2\\ell_{lin}$ to prove $\\nabla ^2\\ell_{lin}$ is a positive definite matrix.\n",
    "$$\n",
    "\\text{det}(\\nabla ^2\\ell_{lin}) = \\frac{1}{10} \\sum_{i=1}^{10} \\frac{y_i^2 e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2} \\frac{1}{10} \\sum_{i=1}^{10} \\frac{y_i^2 x_i^2 e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2} - \\left(\\frac{1}{10} \\sum_{i=1}^{10} \\frac{y_i^2 x_i e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2}\\right)^2\n",
    "$$\n",
    "$$\n",
    "=\\frac{1}{100} \\sum_{i=1}^{10} \\frac{y_i^2 e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2} \\sum_{i=1}^{10}\\frac{y_i^2 x_i^2 e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2} - \\frac{1}{100} \\left(\\sum_{i=1}^{10} \\frac{y_i^2 x_i e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2}\\right)^2\n",
    "$$\n",
    "By the Cauchyâ€“Schwarz inequality, \n",
    "$$\n",
    "\\text{LEFT } =\\left(\\sum_{i=1}^{10} \\frac{y_i^2 x_i e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2}\\right)^2 =\n",
    "\\left(\\sum_{i=1}^{10} \\sqrt{\\frac{y_i^2 e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2}} \\sqrt{\\frac{y_i^2 x_i^2 e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2}} \\right)^2\n",
    "$$\n",
    "$$\n",
    "< \\sum_{i=1}^{10} \\frac{y_i^2 e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2} \\sum_{i=1}^{10} \\frac{y_i^2 x_i^2 e^{-y_i (\\beta_1x_i + \\beta_0)}}{(1+e^{-y_i (\\beta_1x_i + \\beta_0)})^2}= \\text{RIGHT }\n",
    "$$\n",
    "The LEFT = RIGHT equality doesn't hold because \n",
    "$$\n",
    "\\sqrt{\\frac{y_i^2 e^{-y_i (\\beta_1x_i + \\beta_0)}}{\\left(1+e^{-y_i (\\beta_1x_i + \\beta_0)}\\right)^2}} \\not= \\sqrt{\\frac{y_i^2 x_i^2 e^{-y_i (\\beta_1x_i + \\beta_0)}}{\\left(1+e^{-y_i (\\beta_1x_i + \\beta_0)}\\right)^2}} \\text{ for all $i$.}\n",
    "$$\n",
    "Hence\n",
    "$$\n",
    "\\text{det}(\\nabla ^2\\ell_{lin}) > 0 \n",
    "$$\n",
    "\n",
    "Therefore, $\\ell_{log}(\\beta_0,\\beta_1)$ is strictly convex by the Second Order Conditions for Convexity.  \n",
    "\n",
    "**The necessary and sufficient conditions for optimality**  \n",
    "The necessary condition for optimality of $(\\beta_0^\\ast , \\beta_1^\\ast)$ is that if $(\\beta_0^\\ast , \\beta_1^\\ast)$ is a minimiser of $\\ell_{log}(\\beta_0,\\beta_1)$, then $\\nabla \\ell_{log}(\\beta_0^\\ast , \\beta_1^\\ast) = {\\bf 0}$:\n",
    "$$\n",
    "\\frac{1}{10} \\sum_{i=1}^{10}-y_i +\\frac{y_i}{1+e^{-y_i (\\beta_1x_i + \\beta_0)}} = 0 \\text{ and }\n",
    "\\frac{1}{10} \\sum_{i=1}^{10}-y_i x_i +\\frac{y_i x_i}{1+e^{-y_i (\\beta_1x_i + \\beta_0)}} = 0\n",
    "$$\n",
    "The sufficient condition for optimality of $(\\beta_0^\\ast , \\beta_1^\\ast)$ is that if $\\ell_{log}$ is strictly convex, and $\\nabla \\ell_{log}(\\beta_0^\\ast , \\beta_1^\\ast) = {\\bf 0}$:\n",
    "$$\n",
    "\\frac{1}{10} \\sum_{i=1}^{10}-y_i +\\frac{y_i}{1+e^{-y_i (\\beta_1x_i + \\beta_0)}} = 0 \\text{ and }\n",
    "\\frac{1}{10} \\sum_{i=1}^{10}-y_i x_i +\\frac{y_i x_i}{1+e^{-y_i (\\beta_1x_i + \\beta_0)}} = 0\n",
    "$$\n",
    "Then $(\\beta_0^\\ast , \\beta_1^\\ast)$ is the unique minimiser of $\\ell_{log}(\\beta_0,\\beta_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "if $A$ is a symmetric $2$ by $2$ matrix, then any solution to\n",
    "$$\n",
    "\\max_{x\\in\\mathbb{R}^2}\\: \\frac{1}{2}{\\bf x}^T A {\\bf x} \\text{ subject to } ||{\\bf x}||^2 = 1\n",
    "$$\n",
    "is an eigenvector of $A$ corresponding to the largest eigenvalue of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Firstly, we know that \n",
    "$$\n",
    "\\max_{x\\in\\mathbb{R}^2}\\: \\frac{1}{2}{\\bf x}^T A {\\bf x} = \\min_{x\\in\\mathbb{R}^2}\\: -\\frac{1}{2}{\\bf x}^T A {\\bf x}\n",
    "$$\n",
    "Let $f({\\bf x}) = -\\frac{1}{2}{\\bf x}^T A {\\bf x}$, $g({\\bf x}) = ||{\\bf x}||^2 - 1$, and $A = \\begin{pmatrix} \n",
    "a_{(1,1)} & a_{(1,2)} \\\\ \n",
    "a_{(1,2)} & a_{(2,2)}\n",
    "\\end{pmatrix}$, then $f(x_1,x_2) = -\\frac{1}{2}( a_{(1,1)}x_1^2 + 2a_{(1,2)}x_1x_2 + a_{(2,2)}x_2^2 )$.  \n",
    "Suppose ${\\bf x}^\\ast = \\begin{pmatrix} x^\\ast_1 \\\\ x^\\ast_2 \\end{pmatrix}$ is a minimizer of $f({\\bf x})$ subject to the constraint $g({\\bf x}^\\ast) = ||{\\bf x}^\\ast||^2 - 1 = 0$.  \n",
    "So ${\\bf x}^\\ast \\not= {\\bf 0}$ and\n",
    "$$\n",
    "\\nabla g({\\bf x}^\\ast) = \\begin{pmatrix}\n",
    "2x^\\ast_1 \\\\\n",
    "2x^\\ast_2\n",
    "\\end{pmatrix} = 2{\\bf x}^\\ast \\not={\\bf 0}\n",
    "$$\n",
    "By the Theorem (Lagrange Multipliers), there exists a $\\lambda\\in\\mathbb{R}$ such that $\\nabla f({\\bf x}^\\ast) = \\lambda \\nabla g({\\bf x}^\\ast)$.\n",
    "$$\n",
    "f({\\bf x}) = f(x_1,x_2) = \\begin{pmatrix}\n",
    "- a_{(1,1)}x_1 - a_{(1,2)}x_2 \\\\\n",
    "- a_{(1,2)}x_1 - a_{(2,2)}x_2 \n",
    "\\end{pmatrix} = -A{\\bf x}\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\nabla f({\\bf x}^\\ast) = -A{\\bf x}^\\ast = \\lambda \\nabla g({\\bf x}^\\ast) = 2\\lambda {\\bf x}^\\ast \\\\\n",
    "-A{\\bf x}^\\ast = 2\\lambda {\\bf x}^\\ast \\\\\n",
    "A{\\bf x}^\\ast = -2\\lambda {\\bf x}^\\ast\n",
    "$$\n",
    "So $-2\\lambda$ is an eigenvalue of $A$ and ${\\bf x}^\\ast$ is its corresponding eigenvector.   \n",
    "Suppose that $-2\\lambda$ is not the largest eigenvalue of $A$, then there exist a $\\lambda' > -2\\lambda$ and ${\\bf x}'$ such that $A{\\bf x}'=\\lambda'{\\bf x}'$ and $||{\\bf x}'||^2 =1$.  \n",
    "Then \n",
    "$$\n",
    "\\frac{1}{2}{{\\bf x}^\\ast}^T A {\\bf x}^\\ast = \\frac{1}{2}{{\\bf x}^\\ast}^T (-2\\lambda{\\bf x}^\\ast) = -\\lambda ||{\\bf x}^\\ast||^2 = -\\lambda \\\\\n",
    "\\frac{1}{2}{{\\bf x}'}^T A {\\bf x}' = \\frac{1}{2}{{\\bf x}'}^T (\\lambda'{\\bf x}') = \\frac{\\lambda'}{2} ||{\\bf x}'||^2 = \\frac{\\lambda'}{2}\n",
    "$$\n",
    "Since $\\lambda' > -2\\lambda$, then $\\frac{1}{2}{{\\bf x}^\\ast}^T A {\\bf x}^\\ast < \\frac{1}{2}{{\\bf x}'}^T A {\\bf x}'$.  \n",
    "However, we know that ${\\bf x}^\\ast$ is a solution of $\\max_{x\\in\\mathbb{R}^2}\\: \\frac{1}{2}{\\bf x}^T A {\\bf x}$, so we get a contradiction.  \n",
    "Thus ${\\bf x}^\\ast$  is an eigenvector of $A$ corresponding to the largest eigenvalue of $A$, $-2\\lambda$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
