{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLY 561 HW\n",
    "\n",
    "Name:Yuqi Wang   \n",
    "NetID:yw545"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "### Part (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\45336\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Avg Cross Entropy: 0.684698, Gradient Norm: 0.573653, Training Accuracy: 56.8 percent\n",
      "Step: 2, Avg Cross Entropy: 0.684045, Gradient Norm: 0.038587, Training Accuracy: 56.8 percent\n",
      "Step: 4, Avg Cross Entropy: 0.683980, Gradient Norm: 0.013244, Training Accuracy: 56.8 percent\n",
      "Step: 6, Avg Cross Entropy: 0.683967, Gradient Norm: 0.004895, Training Accuracy: 56.8 percent\n",
      "Step: 8, Avg Cross Entropy: 0.683967, Gradient Norm: 0.000959, Training Accuracy: 56.8 percent\n",
      "Step: 10, Avg Cross Entropy: 0.683966, Gradient Norm: 0.001043, Training Accuracy: 56.8 percent\n",
      "Step: 12, Avg Cross Entropy: 0.683965, Gradient Norm: 0.000915, Training Accuracy: 56.8 percent\n",
      "Step: 14, Avg Cross Entropy: 0.683965, Gradient Norm: 0.001049, Training Accuracy: 56.8 percent\n",
      "Step: 16, Avg Cross Entropy: 0.683964, Gradient Norm: 0.000924, Training Accuracy: 56.8 percent\n",
      "Step: 18, Avg Cross Entropy: 0.683964, Gradient Norm: 0.001086, Training Accuracy: 56.8 percent\n",
      "Step: 20, Avg Cross Entropy: 0.683963, Gradient Norm: 0.000943, Training Accuracy: 56.8 percent\n",
      "Step: 22, Avg Cross Entropy: 0.683963, Gradient Norm: 0.002028, Training Accuracy: 56.8 percent\n",
      "Step: 24, Avg Cross Entropy: 0.683962, Gradient Norm: 0.001797, Training Accuracy: 56.8 percent\n",
      "Step: 26, Avg Cross Entropy: 0.683962, Gradient Norm: 0.001586, Training Accuracy: 56.8 percent\n",
      "Step: 28, Avg Cross Entropy: 0.683961, Gradient Norm: 0.001398, Training Accuracy: 56.8 percent\n",
      "Step: 30, Avg Cross Entropy: 0.683961, Gradient Norm: 0.001235, Training Accuracy: 56.8 percent\n",
      "Step: 32, Avg Cross Entropy: 0.683960, Gradient Norm: 0.001011, Training Accuracy: 56.8 percent\n",
      "Step: 34, Avg Cross Entropy: 0.683960, Gradient Norm: 0.000896, Training Accuracy: 56.8 percent\n",
      "Step: 36, Avg Cross Entropy: 0.683959, Gradient Norm: 0.001099, Training Accuracy: 56.8 percent\n",
      "Step: 38, Avg Cross Entropy: 0.683959, Gradient Norm: 0.000935, Training Accuracy: 56.8 percent\n",
      "Step: 40, Avg Cross Entropy: 0.683958, Gradient Norm: 0.000846, Training Accuracy: 56.8 percent\n",
      "Step: 42, Avg Cross Entropy: 0.683958, Gradient Norm: 0.001008, Training Accuracy: 56.8 percent\n",
      "Step: 44, Avg Cross Entropy: 0.683957, Gradient Norm: 0.000876, Training Accuracy: 56.8 percent\n",
      "Step: 46, Avg Cross Entropy: 0.683957, Gradient Norm: 0.001062, Training Accuracy: 56.8 percent\n",
      "Step: 48, Avg Cross Entropy: 0.683956, Gradient Norm: 0.000893, Training Accuracy: 56.8 percent\n",
      "Step: 50, Avg Cross Entropy: 0.683956, Gradient Norm: 0.001086, Training Accuracy: 56.8 percent\n",
      "Step: 52, Avg Cross Entropy: 0.683955, Gradient Norm: 0.000893, Training Accuracy: 56.8 percent\n",
      "Step: 54, Avg Cross Entropy: 0.683955, Gradient Norm: 0.001743, Training Accuracy: 56.8 percent\n",
      "Step: 56, Avg Cross Entropy: 0.683954, Gradient Norm: 0.001269, Training Accuracy: 56.8 percent\n",
      "Step: 58, Avg Cross Entropy: 0.683954, Gradient Norm: 0.001875, Training Accuracy: 56.8 percent\n",
      "Step: 60, Avg Cross Entropy: 0.683953, Gradient Norm: 0.001297, Training Accuracy: 56.8 percent\n",
      "Step: 62, Avg Cross Entropy: 0.683953, Gradient Norm: 0.001846, Training Accuracy: 56.8 percent\n",
      "Step: 64, Avg Cross Entropy: 0.683952, Gradient Norm: 0.001231, Training Accuracy: 56.8 percent\n",
      "Step: 66, Avg Cross Entropy: 0.683952, Gradient Norm: 0.000910, Training Accuracy: 56.8 percent\n",
      "Step: 68, Avg Cross Entropy: 0.683951, Gradient Norm: 0.001729, Training Accuracy: 56.8 percent\n",
      "Step: 70, Avg Cross Entropy: 0.683951, Gradient Norm: 0.001118, Training Accuracy: 56.8 percent\n",
      "Step: 72, Avg Cross Entropy: 0.683950, Gradient Norm: 0.000848, Training Accuracy: 56.8 percent\n",
      "Step: 74, Avg Cross Entropy: 0.683950, Gradient Norm: 0.000946, Training Accuracy: 56.8 percent\n",
      "Step: 76, Avg Cross Entropy: 0.683949, Gradient Norm: 0.001795, Training Accuracy: 56.8 percent\n",
      "Step: 78, Avg Cross Entropy: 0.683949, Gradient Norm: 0.002188, Training Accuracy: 56.8 percent\n",
      "Step: 80, Avg Cross Entropy: 0.683948, Gradient Norm: 0.001184, Training Accuracy: 56.8 percent\n",
      "Step: 82, Avg Cross Entropy: 0.683948, Gradient Norm: 0.000836, Training Accuracy: 56.8 percent\n",
      "Step: 84, Avg Cross Entropy: 0.683947, Gradient Norm: 0.000895, Training Accuracy: 56.8 percent\n",
      "Step: 86, Avg Cross Entropy: 0.683947, Gradient Norm: 0.001522, Training Accuracy: 56.8 percent\n",
      "Step: 88, Avg Cross Entropy: 0.683946, Gradient Norm: 0.001614, Training Accuracy: 56.8 percent\n",
      "Step: 90, Avg Cross Entropy: 0.683946, Gradient Norm: 0.001647, Training Accuracy: 56.8 percent\n",
      "Step: 92, Avg Cross Entropy: 0.683945, Gradient Norm: 0.001619, Training Accuracy: 56.8 percent\n",
      "Step: 94, Avg Cross Entropy: 0.683944, Gradient Norm: 0.001536, Training Accuracy: 56.8 percent\n",
      "Step: 96, Avg Cross Entropy: 0.683944, Gradient Norm: 0.001414, Training Accuracy: 56.8 percent\n",
      "Step: 98, Avg Cross Entropy: 0.683943, Gradient Norm: 0.001273, Training Accuracy: 56.8 percent\n",
      "Final test accuracy: 76.9 percent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "def chain_rule(Dg, Df, var_shape):\n",
    "    # Computes the Jacobian D (g o f)\n",
    "    dim = len(var_shape)\n",
    "    Dg_axes = list(range(Dg.ndim-dim, Dg.ndim))\n",
    "    Df_axes = list(range(dim))\n",
    "    return np.tensordot(Dg, Df, axes=(Dg_axes, Df_axes))\n",
    "\n",
    "# Compute the Jacobian blocks of X @ W + b\n",
    "\n",
    "def DX_affine(X, W, b):\n",
    "    # (d_{x_{i, j}} (X @ W))_{a, b} = e_a^T e_ie_j^T W e_b, so a,i slices equal W.T\n",
    "    D = np.zeros((X.shape[0], W.shape[1], X.shape[0], X.shape[1]))\n",
    "    for k in range(X.shape[0]):\n",
    "        D[k,:,k,:]=W.T\n",
    "    return D, X.shape\n",
    "\n",
    "def DW_affine(X, W, b):\n",
    "    # (d_{w_{i, j}} (X @ W))_{a, b} = e_a^T X e_ie_j^T e_b, so b, j slices equal x\n",
    "    D = np.zeros((X.shape[0], W.shape[1], W.shape[0], W.shape[1]))\n",
    "    for k in range(W.shape[1]):\n",
    "        D[:,k,:,k]=X\n",
    "    return D, W.shape\n",
    "\n",
    "def Db_affine(X, W, b):\n",
    "    # (d_{b_i} (1 @ b))_{a, b} = e_a^T 1 e_i^T e_b, so b, i slices are all ones\n",
    "    D = np.zeros((X.shape[0], W.shape[1], b.shape[1]))\n",
    "    for k in range(b.shape[1]):\n",
    "        D[:,k,k]=1\n",
    "    return D, b.shape\n",
    "    \n",
    "def logit(z):\n",
    "    # This is vectorized\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def Dlogit(Z):\n",
    "    # The Jacobian of the matrix logit\n",
    "    D = np.zeros((Z.shape[0], Z.shape[1], Z.shape[0], Z.shape[1]))\n",
    "    A = logit(Z) * logit(-Z)\n",
    "    for i in range(Z.shape[0]):\n",
    "        for j in range(Z.shape[1]):\n",
    "            D[i,j,i,j] = A[i,j]\n",
    "    return D, Z.shape\n",
    "\n",
    "def softmax(z):\n",
    "    v = np.exp(z)\n",
    "    return v / np.sum(v)\n",
    "\n",
    "def matrix_softmax(Z):\n",
    "    return np.apply_along_axis(softmax, 1, Z)\n",
    "\n",
    "def Dmatrix_softmax(Z):\n",
    "    D = np.zeros((Z.shape[0], Z.shape[1], Z.shape[0], Z.shape[1]))\n",
    "    for k in range(Z.shape[0]):\n",
    "        v = np.exp(Z[k,:])\n",
    "        v = v / np.sum(v)\n",
    "        D[k,:,k,:] = np.diag(v) - np.outer(v,v)\n",
    "        #print(D[k,:,k,:])\n",
    "    return D, Z.shape\n",
    "\n",
    "def cross_entropy(P, Q):\n",
    "    return -np.sum(P * np.log(Q))/P.shape[0]\n",
    "\n",
    "def DQcross_entropy(P, Q):\n",
    "    return - P * (1/Q)/P.shape[0], Q.shape\n",
    "\n",
    "def nn_loss_closure(X, Y):\n",
    "    # vars[0]=W_1, vars[1]=b_1, vars[2]=W_2, vars[3]=b_2\n",
    "    # cross_entropy(Y, matrix_softmax(affine(logit(affine(X; W_1, b_1))); W_2, b_2))\n",
    "    def f(var):\n",
    "        return cross_entropy(Y, matrix_softmax((logit((logit((X @ var[0]) + var[1]) @  var[2]) + var[3]) @  var[4]) + var[5]))\n",
    "    return f\n",
    "\n",
    "def nn_loss_gradient_closure(X, Y):\n",
    "    def df(var):\n",
    "        # Activation of first layer\n",
    "        Z1 = (X @ var[0]) + var[1]\n",
    "        X2 = logit(Z1)\n",
    "        \n",
    "        # Activation of second layer\n",
    "        Z2 = (X2 @ var[2]) + var[3]\n",
    "        X3 = logit(Z2)\n",
    "        \n",
    "        # Activation of third layer\n",
    "        Z3 = (X3 @ var[4]) + var[5]\n",
    "        Q = matrix_softmax(Z3)\n",
    "        \n",
    "        # Backpropagation tells us we can immediately contract DQ DZ3\n",
    "        D_Q, Qshape = DQcross_entropy(Y, Q)\n",
    "        D_Z3, Z3shape = Dmatrix_softmax(Z3)\n",
    "        back_prop3 = chain_rule(D_Q, D_Z3, Qshape)\n",
    "        \n",
    "        # Jacobians for phi_3\n",
    "        \n",
    "        D_X3, X3shape = DX_affine(X3, var[4], var[5])\n",
    "        D_W3, W3shape = DW_affine(X3, var[4], var[5])\n",
    "        D_b3, b3shape = Db_affine(X3, var[4], var[5])\n",
    "        \n",
    "    \n",
    "        D_X2, X2shape = DX_affine(X2, var[2], var[3])\n",
    "        \n",
    "        # Jacobian for psi_1\\2\n",
    "        D_Z1, Z1shape = Dlogit(Z1)\n",
    "        D_Z2, Z2shape = Dlogit(Z2)\n",
    "        \n",
    "        \n",
    "        back_prop2 = chain_rule(chain_rule(back_prop3, D_X3, X3shape), D_Z2, Z2shape)\n",
    "        back_prop1 = chain_rule(chain_rule(back_prop2, D_X2, X2shape), D_Z1, Z1shape)\n",
    "        \n",
    "        # Jacobians for phi_1\n",
    "        D_W1, W1shape = DW_affine(X, var[0], var[1])\n",
    "        D_b1, b1shape = Db_affine(X, var[0], var[1])\n",
    "        \n",
    "        # Jacobians for phi_2\n",
    "        D_W2, W2shape = DW_affine(X2, var[2], var[3])\n",
    "        D_b2, b2shape = Db_affine(X2, var[2], var[3])\n",
    "        \n",
    "        # Compute all the gradients\n",
    "        W1grad = chain_rule(back_prop1, D_W1, W1shape)\n",
    "        b1grad = chain_rule(back_prop1, D_b1, b1shape)\n",
    "        W2grad = chain_rule(back_prop2, D_W2, W2shape)\n",
    "        b2grad = chain_rule(back_prop2, D_b2, b2shape)\n",
    "        W3grad = chain_rule(back_prop3, D_W3, W3shape)\n",
    "        b3grad = chain_rule(back_prop3, D_b3, b3shape)\n",
    "        \n",
    "        return [W1grad, b1grad, W2grad, b2grad, W3grad, b3grad]\n",
    "    return df\n",
    "\n",
    "def update_blocks(x,y,t):\n",
    "    # An auxiliary function for backtracking with blocks of variables\n",
    "    num_blocks = len(x)\n",
    "    z = [None]*num_blocks\n",
    "    for i in range(num_blocks):\n",
    "        z[i] = x[i] + t*y[i]\n",
    "    return z\n",
    "                           \n",
    "def block_backtracking(x0, f, dx, df0, alpha=0.1, beta=0.5, verbose=False):\n",
    "    num_blocks = len(x0)\n",
    "    \n",
    "    delta = 0\n",
    "    for i in range(num_blocks):\n",
    "        delta = delta + np.sum(dx[i] * df0[i])\n",
    "    delta = alpha * delta\n",
    "    \n",
    "    f0 = f(x0)\n",
    "    \n",
    "    t = 1\n",
    "    x = update_blocks(x0, dx, t)\n",
    "    fx = f(x)\n",
    "    while (not np.isfinite(fx)) or f0+t*delta<fx:\n",
    "        t = beta*t\n",
    "        x = update_blocks(x0, dx, t)\n",
    "        fx = f(x)\n",
    "        \n",
    "    if verbose:\n",
    "        print((t, delta))\n",
    "        l=-1e-5\n",
    "        u=1e-5\n",
    "        s = np.linspace(l, u, 64)\n",
    "        fs = np.zeros(s.size)\n",
    "        crit = f0 + s*delta\n",
    "        tan = f0 + s*delta/alpha\n",
    "        for i in range(s.size):\n",
    "            fs[i] = f(update_blocks(x0, dx, s[i]))\n",
    "        plt.plot(s, fs)\n",
    "        plt.plot(s, crit, '--')\n",
    "        plt.plot(s, tan, '.')\n",
    "        plt.scatter([0], [f0])\n",
    "        plt.show()\n",
    "            \n",
    "    return x, fx\n",
    "\n",
    "def negate_blocks(x):\n",
    "    # Helper function for negating the gradient of block variables\n",
    "    num_blocks = len(x)\n",
    "    z = [None]*num_blocks\n",
    "    for i in range(num_blocks):\n",
    "        z[i] = -x[i]\n",
    "    return z\n",
    "\n",
    "def block_norm(x):\n",
    "    num_blocks=len(x)\n",
    "    z = 0\n",
    "    for i in range(num_blocks):\n",
    "        z = z + np.sum(x[i]**2)\n",
    "    return np.sqrt(z)\n",
    "\n",
    "def random_matrix(shape, sigma=0.1):\n",
    "    # Helper for random initialization\n",
    "    return np.reshape(sigma*rd.randn(shape[0]*shape[1]), shape)\n",
    "\n",
    "### Begin gradient descent example\n",
    "\n",
    "### Random seed\n",
    "rd.seed(1234)\n",
    "\n",
    "data = load_breast_cancer() # Loads the Wisconsin Breast Cancer dataset (569 examples in 30 dimensions)\n",
    "\n",
    "# Parameters for the data\n",
    "dim_data = 30\n",
    "num_labels = 2\n",
    "num_examples = 569\n",
    "\n",
    "# Parameters for training\n",
    "num_train = 400\n",
    "\n",
    "X = data['data'] # Data in rows\n",
    "targets = data.target # 0-1 labels\n",
    "labels = np.zeros((num_examples, num_labels))\n",
    "for i in range(num_examples):\n",
    "    labels[i,targets[i]]=1 # Conversion to one-hot representations\n",
    "\n",
    "# Prepare hyperparameters of the network\n",
    "hidden_nodes = 20\n",
    "\n",
    "# Initialize variables\n",
    "W1_init = random_matrix((dim_data, hidden_nodes))\n",
    "b1_init = np.zeros((1, hidden_nodes))\n",
    "\n",
    "W12_init = random_matrix((hidden_nodes, hidden_nodes))\n",
    "b12_init = np.zeros((1, hidden_nodes))\n",
    "\n",
    "W2_init = random_matrix((hidden_nodes, num_labels))\n",
    "b2_init = np.zeros((1, num_labels))\n",
    "\n",
    "x = [W1_init, b1_init, W12_init, b12_init, W2_init, b2_init]\n",
    "f = nn_loss_closure(X[:num_train,:], labels[:num_train,:])\n",
    "df = nn_loss_gradient_closure(X[:num_train,:], labels[:num_train,:])\n",
    "dx = lambda v: negate_blocks(df(v))\n",
    "    \n",
    "for i in range(100):\n",
    "    ngrad = dx(x)\n",
    "    x, fval = block_backtracking(x, f, ngrad, df(x), alpha=0.1, verbose=False)\n",
    "    \n",
    "    train_data = matrix_softmax(logit(logit(X[:num_train,:]@x[0] + x[1])@x[2] +x[3]) @ x[4] + x[5])\n",
    "    train_labels = np.argmax(train_data, axis=1)\n",
    "    per_correct = 100*(1 - np.count_nonzero(train_labels - targets[:num_train])/num_train)\n",
    "\n",
    "    if i % 2 == 0:\n",
    "        print(\"Step: %d, Avg Cross Entropy: %f, Gradient Norm: %f, Training Accuracy: %.1f percent\" % (i,fval,block_norm(ngrad), per_correct))\n",
    "    \n",
    "test_data = matrix_softmax(logit(logit(X[num_train:,:]@x[0] + x[1])@x[2] +x[3]) @ x[4] + x[5])\n",
    "test_labels = np.argmax(test_data, axis=1)\n",
    "per_correct = 100*(1 - np.count_nonzero(test_labels - targets[num_train:])/(num_examples-num_train))\n",
    "\n",
    "print('Final test accuracy: %.1f percent' % per_correct)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After about $100$ steps, the final test accuracy is around $76.9$%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)\n",
    "\n",
    "Three ways to make this implementation more efficient:  \n",
    "\n",
    "1. The Jacobian matrix is very sparse, so we may find a package about sparse matrix and then use the package to store the Jacobian in Python instead of storing as a full matrix. For example, we can find a package which allows us to only store data positions where the data entry is nonzero.  \n",
    "\n",
    "2. Besides, we can write an outer function for the affine function in order to reduce the number of indices which chain rule requires summation over.  \n",
    "\n",
    "3. In addition, we can try other methods. In stead of Gradient Descent, we can use Stochastic Gradient Descent in order to try to jump out of the local minimum.  \n",
    "\n",
    "4. Finally, we may reduce the size of training data to make this implementation more efficient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "### Part (a)  \n",
    "\n",
    "We have  \n",
    "$$\n",
    "X=\\begin{pmatrix}\n",
    "x_{1,1} & x_{1,2} & x_{1,3}\\\\\n",
    "x_{2,1} & x_{2,2} & x_{2,3}\\\\\n",
    "x_{3,1} & x_{3,2} & x_{3,3}\n",
    "\\end{pmatrix}\n",
    "$$   \n",
    "We set  \n",
    "$$\n",
    "Y=\\begin{pmatrix}\n",
    "y_{1,1} & y_{1,2}\\\\\n",
    "y_{2,1} & y_{2,2}\n",
    "\\end{pmatrix}=X \\star \\begin{pmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}= c_{(i, j)}(X,\\mathcal{T})_{{\\bf k}_{\\setminus\\{i\\}}\\oplus{\\bf l}_{\\setminus\\{j\\}}} \n",
    "$$  \n",
    "where $\\mathcal{T}$ is a 2 by 2 by 3 by 3 tensor. As a form of constraction, we have  \n",
    "$$\n",
    "y_{1,1}=\\sum_{i=1}^{3}\\sum_{j=1}^{3}x_{i,j}\\mathcal{T}_{1,2,i,j}= x_{1,1}+x_{2,2}\n",
    "$$  \n",
    "So   \n",
    "$$\n",
    "\\mathcal{T}_{1,1,\\cdot,\\cdot}=\\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "$$  \n",
    "Similarly, we can get   \n",
    "$$\n",
    "\\mathcal{T}_{1,2,\\cdot,\\cdot}=\\begin{pmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix} \\\\\n",
    "\\mathcal{T}_{2,1,\\cdot,\\cdot}=\\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{pmatrix} \\\\\n",
    "\\mathcal{T}_{2,2,\\cdot,\\cdot}=\\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "$$  \n",
    "Thus,\n",
    "$$\n",
    "\\mathcal{T}=\\left(\\left(\\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}, \\begin{pmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}\\right),\\left(\\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{pmatrix}, \\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\\right)\\right)\n",
    "$$\n",
    "And contraction should occur between $X$ and $\\mathcal{T}$ along $i=\\{1,2\\}$, $j=\\{3,4\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)\n",
    "\n",
    "We have  \n",
    "$$\n",
    "X=\\left(\\begin{pmatrix}\n",
    "x_{1,1,1} & x_{1,1,2} & x_{1,1,3}\\\\\n",
    "x_{1,2,1} & x_{1,2,2} & x_{1,2,3}\\\\\n",
    "x_{1,3,1} & x_{1,3,2} & x_{1,3,3}\n",
    "\\end{pmatrix},\\begin{pmatrix}\n",
    "x_{2,1,1} & x_{2,1,2} & x_{2,1,3}\\\\\n",
    "x_{2,2,1} & x_{2,2,2} & x_{2,2,3}\\\\\n",
    "x_{2,3,1} & x_{2,3,2} & x_{2,3,3}\n",
    "\\end{pmatrix}\\right)\n",
    "$$   \n",
    "We set  \n",
    "$$\n",
    "Y=\\begin{pmatrix}\n",
    "y_{1,1} & y_{1,2}\\\\\n",
    "y_{2,1} & y_{2,2}\n",
    "\\end{pmatrix}=X \\star \\mathcal{H}= X \\star \\left(\\begin{pmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{pmatrix},\\begin{pmatrix}\n",
    "1 & 1\\\\\n",
    "1 & 1\n",
    "\\end{pmatrix}\\right) = c_{(i, j)}(X,\\mathcal{T})_{{\\bf k}_{\\setminus\\{i\\}}\\oplus{\\bf l}_{\\setminus\\{j\\}}} \n",
    "$$   \n",
    "where $\\mathcal{T}$ is a 2 by 2 by 2 by 3 by 3 tensor. As a form of constraction, we have   \n",
    "$$\n",
    "y_{1,1}=\\sum_{i=1}^{2}\\sum_{j=1}^{3}\\sum_{k=1}^{3}x_{i,j,k}\\mathcal{T}_{1,1,i,j,k}= x_{1,1,1}+x_{1,2,2}+x_{2,1,1}+x_{2,1,2}+x_{2,1,3}+x_{2,2,1}+x_{2,2,2}\n",
    "$$   \n",
    "So   \n",
    "$$\n",
    "\\mathcal{T}_{1,1,\\cdot,\\cdot,\\cdot}=\\left(\\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix},\\begin{pmatrix}\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}\\right)\n",
    "$$  \n",
    "Similarly, we can get   \n",
    "$$\n",
    "\\mathcal{T}_{1,2,\\cdot,\\cdot,\\cdot}=\\left(\\begin{pmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix},\\begin{pmatrix}\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}\\right) \\\\\n",
    "\\mathcal{T}_{2,1,\\cdot,\\cdot,\\cdot}=\\left(\\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{pmatrix},\\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 1 & 0\n",
    "\\end{pmatrix}\\right) \\\\\n",
    "\\mathcal{T}_{2,1,\\cdot,\\cdot,\\cdot}=\\left(\\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix},\\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}\\right)\n",
    "$$   \n",
    "Thus,  \n",
    "$$\n",
    "\\mathcal{T}=\\left(\\left(\\left(\\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix},\\begin{pmatrix}\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}\\right),\\left(\\begin{pmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix},\\begin{pmatrix}\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}\\right)\\right),\\left(\\left(\\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{pmatrix},\\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 1 & 0\n",
    "\\end{pmatrix}\\right),\\left(\\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix},\\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}\\right)\\right)\\right)\n",
    "$$  \n",
    "\n",
    "And contraction should occur between $X$ and $\\mathcal{T}$ along $i=\\{1,2,3\\}$, $j=\\{3,4,5\\}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "**Weather Based Merchandise Inventory Optimization**\n",
    "\n",
    "Outline:\n",
    "\n",
    "Title: Using machine learning to predict demand for weather-sensitive products at Walmart Stores\n",
    "\n",
    "Thesis: This paper is going to discuss how major weather events affect the sales of potentially weather-sensitive products at different Walmart stores. And in order to optimize prediction accuracy of the sales volume,  several supervised and unsupervised machine learning methods were applied in this study, including regression, different models to a historical datasets from Walmart to find the inner pattern for products sales giving certain weather condition. A portion of original data sets will be randomly selected for each store as the test data set before building our model to evaluate the results. Based on this inner pattern, we hope to help the market to optimize its sales for weather-sensitive products by offering strategies such as product-bundling.\n",
    "\n",
    "\n",
    "I. Introduction\n",
    "\n",
    "    A. Background introduction and overview\n",
    "    \n",
    "    B. The previous researches of prediction for product sales based on weather conditions.\n",
    "\n",
    "II. Exploratory Data Analysis\n",
    "\n",
    "    A. Describe the details and selections of attributes in our data used in this project.\n",
    "    \n",
    "    B. Analyze the data - do some basic statistical analyses.\n",
    "    \n",
    "    C. Clean the raw datasets - remove all zero entries.\n",
    "    \n",
    "    D. Transformed or manipulated the datasets for further analysis - merge three datasets based on Date.\n",
    "\n",
    "\n",
    "\n",
    "III. Approaches used to analyze the datasets\n",
    "\n",
    "    A. SVM\n",
    "    \n",
    "        1. Introduction to SVM\n",
    "        2. How to use SVM on these datasets\n",
    "        3. Advantage and disadvantages\n",
    "    \n",
    "    B. Decision tree\n",
    "    \n",
    "        1. Introduction to Decision tree\n",
    "        2. How to use Decision tree on these datasets\n",
    "        3. Advantage and disadvantages\n",
    "    \n",
    "    C. Neural Network\n",
    "    \n",
    "        1. Introduction to Neural Network\n",
    "        2. How to use Neural Network on these datasets\n",
    "        3. Advantage and disadvantages\n",
    "    \n",
    "    D. Regression\n",
    "    \n",
    "        1. Introduction to Regression\n",
    "        2. How to use Regression on these datasets\n",
    "        3. Advantage and disadvantages\n",
    "\n",
    "\n",
    "IV. Prediction analysis\n",
    "\n",
    "\n",
    "    A. Explain how to use the Root Mean Squared Logarithmic Error (RMSLE) to test prediction accuracy\n",
    "    \n",
    "    B. Showing results for different methods\n",
    "    \n",
    "        1. SVM\n",
    "        2. Decision tree\n",
    "        3. Neural Network\n",
    "        4. Regression\n",
    "\n",
    "\n",
    "V. Discussion\n",
    "    \n",
    "    Discuss the results.\n",
    "\n",
    "VI. References\n",
    "\n",
    "\n",
    "\n",
    "VII. Appendix\n",
    "\n",
    "    Visualizations\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
